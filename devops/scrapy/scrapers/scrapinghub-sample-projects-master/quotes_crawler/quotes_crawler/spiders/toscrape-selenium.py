# -*- coding:utf-8 -*-
import scrapy
from selenium import webdriver


# this spider needs PhantomJS (http://phantomjs.org/) installed somewhere in your PATH
class ToScrapeSeleniumSpider(scrapy.Spider):
    name = 'toscrape-selenium'
    start_urls = [
        'http://quotes.toscrape.com/js'
    ]

    def __init__(self, *args, **kwargs):
        self.driver = webdriver.PhantomJS()
        super(ToScrapeSeleniumSpider, self).__init__(*args, **kwargs)

    def parse(self, response):
        self.driver.get(response.url)
        for quote in self.driver.find_elements_by_css_selector('div.quote'):
            yield {
                'quote': quote.find_element_by_css_selector("span.text").text,
                'author': quote.find_element_by_css_selector("small.author").text,
                'tags': [e.text for e in quote.find_elements_by_class_name('tag')],
            }
        # pagination links are not generated by JS code in this page
        next_page_url = response.css("li.next > a::attr(href)").extract_first()
        if next_page_url is not None:
            yield scrapy.Request(response.urljoin(next_page_url))
